# -*- coding: utf-8 -*-
"""streamlit

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wPV8l3l61oQ0ATnpDtlCHuPKqd7-3cpE
"""

from google.colab import drive
drive.mount('/content/drive')

import nltk
nltk.download('stopwords')

import torch

# Get the GPU
if torch.cuda.is_available():        
    device = torch.device("cuda")
else:
    device = torch.device("cpu")
print(device)

# !pip install streamlit

import streamlit as st
import pandas as pd
import numpy as np
import nltk
import gensim
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import tensorflow as tf
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from wordcloud import WordCloud, STOPWORDS
from nltk import word_tokenize
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Bidirectional
from tensorflow.keras.models import Model
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
import torch
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

# !pip install transformers

from transformers import BertConfig, BertModel

from transformers import RobertaTokenizer

# Load the BERT tokenizer.
tokenizer = RobertaTokenizer.from_pretrained('roberta-large', do_lower_case=True)

st.title("Fake News Classifier")
#st.sidebar.title("News Visualization Tools")

stop_words = stopwords.words("english")
stop_words.extend(['from','subject','re','edu','use'])

def preprocess(text):
  result = []
  for token in gensim.utils.simple_preprocess(text):
    if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:
      result.append(token)
  return result

# @st.cache(allow_output_mutation=True)
def fetch_data():
  model = torch.load('/content/drive/My Drive/ire_project/model_save/pytorch_model.pt')
  return model

model = fetch_data()

def convert_input(user_input):
	
		input_ids = []
		attention_masks = []

		# For every sentence...
		# for sent in sentences:
		encoded_dict = tokenizer.encode_plus(
												user_input,                      # Sentence to encode.
												add_special_tokens = True, # Add '[CLS]' and '[SEP]'
												max_length = 24,           # Pad & truncate all sentences.
												pad_to_max_length = True,
												return_attention_mask = True,   # Construct attn. masks.
												return_tensors = 'pt',     # Return pytorch tensors.
												truncation=True,
									)
				
		# Add the encoded sentence to the list.    
		input_ids.append(encoded_dict['input_ids'])

		# And its attention mask (simply differentiates padding from non-padding).
		attention_masks.append(encoded_dict['attention_mask'])

		# Convert the lists into tensors.
		input_ids = torch.cat(input_ids, dim=0)
		attention_masks = torch.cat(attention_masks, dim=0)
		# labels = torch.tensor(labels)

		# Print sentence 0, now as a list of IDs.
		print('Original: ',	user_input[0])
		print('Token IDs:', input_ids[0])
		dataset = TensorDataset(input_ids, attention_masks)	
		prediction_data = dataset
		prediction_sampler = SequentialSampler(prediction_data)
		prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=32)
		return prediction_dataloader

def predict_value(model,prediction_dataloader):
		predictions , true_labels = [], []

		# Predict 
		for batch in prediction_dataloader:
			# Add batch to GPU
			batch = tuple(t.to(device) for t in batch)
			
			# Unpack the inputs from our dataloader
			b_input_ids, b_input_mask = batch
			
			# Telling the model not to compute or store gradients, saving memory and 
			# speeding up prediction
			with torch.no_grad():
					# Forward pass, calculate logit predictions
					outputs = model(b_input_ids, token_type_ids=None, 
													attention_mask=b_input_mask)
			print(outputs)
			logits = outputs[0]

			# Move logits and labels to CPU
			logits = logits.detach().cpu().numpy()
			# label_ids = b_labels.to('cpu').numpy()
			
			# Store predictions and true labels
			predictions.append(logits)
			# true_labels.append(label_ids)
		return predictions


# ------------------------------------------------------------------------------------------------------------ #
import pickle

filename = '/content/drive/My Drive/ire_project/Colab Notebooks/saved_models/tfidf_model.sav'
tfidf_model = pickle.load(open(filename, 'rb'))

filename = '/content/drive/My Drive/ire_project/Colab Notebooks/saved_models/tfidf_v.sav'
tfidf_v = pickle.load(open(filename, 'rb'))

import re
from spacy.lang.en.stop_words import STOP_WORDS


from Stemmer import Stemmer
stemmer = Stemmer('porter')

from sklearn.feature_extraction.text import TfidfVectorizer

def process_input_body(user_input):
    corpus = []
    review = re.sub('[^a-zA-Z]', ' ', user_input)
    review = review.lower()
    review = review.split()
    
    review = [stemmer.stemWord(word) for word in review if not word in STOP_WORDS]
    review = ' '.join(review)

    corpus.append(review)
    return corpus


def handle_input_body(user_input):
    processed_inp = process_input_body(user_input)
    print(processed_inp)
    print(type(processed_inp))

    # TFidf Vectorizer
    # tfidf_v=TfidfVectorizer(max_features=6000)
    vectorized_inp=tfidf_v.transform(processed_inp).toarray()

    print(vectorized_inp)
    print(type(vectorized_inp))
    print(vectorized_inp.shape)

    y_pred = tfidf_model.predict(vectorized_inp)
    return y_pred

# ------------------------------------------------------------------------------------------------------------ #
user_input_title = st.text_area("Enter title")
# st.button('Submit Title')

if st.button("SubmitTitle"):
	loader = convert_input(user_input_title)
	# print(type(processed_input))
	prediction = predict_value(model, loader)
	value = np.max(prediction, axis=1).flatten()
	prediction_value = np.argmax(value)
	if prediction_value == 1:
		st.markdown("## Oh No!: Title detected as Fake News üëé")
		st.write(user_input_title)
		st.write("THIS IS FAKE")
	else:
		st.markdown("## Hurrah: Title detected as Real News üëç")
		st.write(user_input_title)
		st.write("THIS IS REAL")

user_input_body = st.text_area("Enter body")
# st.button('Submit Body')
# user_input = "hey i exist"

# print(user_input_body)



if st.button("SubmitBody"):
	prediction_value = handle_input_body(user_input_body)
	if prediction_value == 1:                                     
		st.markdown("## Oh No!: Body detected as Fake News üëé")
		st.write(user_input_body)
		st.write("THIS IS FAKE")
	else:
		st.markdown("## Hurrah: Body detected as Real News üëç")
		st.write(user_input_body)
		st.write("THIS IS REAL")


# # Classifying article title
# if user_input_title !="":
# 	loader = convert_input(user_input_title)
# 	# print(type(processed_input))
# 	prediction = predict_value(model, loader)
# 	value = np.max(prediction, axis=1).flatten()
# 	prediction_value = np.argmax(value)
# 	if prediction_value == 1:
# 		st.markdown("## Oh No!: Title detected as Fake News üëé")
# 		st.write(user_input_title)
# 		st.write("THIS IS FAKE")
# 	else:
# 		st.markdown("## Hurrah: Title detected as Real News üëç")
# 		st.write(user_input_title)
# 		st.write("THIS IS REAL")


# # Classifying article body
# if user_input_body != "":
# 	prediction_value = handle_input_body(user_input_body)
# 	if prediction_value == 1:                                     
# 		st.markdown("## Oh No!: Body detected as Fake News üëé")
# 		st.write(user_input_body)
# 		st.write("THIS IS FAKE")
# 	else:
# 		st.markdown("## Hurrah: Body detected as Real News üëç")
# 		st.write(user_input_body)
# 		st.write("THIS IS REAL")


